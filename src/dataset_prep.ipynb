{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer, AdamW,   get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional.classification import auroc\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 id  ... identity_hate\n",
       "0  0000997932d777bf  ...             0\n",
       "1  000103f0d9cfb60f  ...             0\n",
       "2  000113f07ec002fd  ...             0\n",
       "3  0001b41b1c6bb37e  ...             0\n",
       "4  0001d958c54c6e35  ...             0\n",
       "\n",
       "[5 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = df.columns[2:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((151592, 8), (7979, 8))"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "33214"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "train_df[LABEL_COLUMNS].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3873      0\n",
       "105730    0\n",
       "20338     0\n",
       "157397    0\n",
       "78552     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "train_df[LABEL_COLUMNS].sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic = train_df[train_df[LABEL_COLUMNS].sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15378, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "train_toxic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train_df[train_df[LABEL_COLUMNS].sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(136214, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([\n",
    "    train_toxic,\n",
    "    train_clean.sample(15_000)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30378, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "toxic            14500\n",
       "severe_toxic      1512\n",
       "obscene           7987\n",
       "threat             443\n",
       "insult            7434\n",
       "identity_hate     1338\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "train_df[LABEL_COLUMNS].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"bert-base-cased\"\n",
    "TOKENIZER = BertTokenizer.from_pretrained(BERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('\"\\n\\n Vandalism on King\\'s Daughters by  \\n\\nI had to ask your help on this about seven weeks ago when this user regularly trolled the discussion page with ethnically insulting remarks.  You put up a warning that you\\'d block the entire range if they continued trolling.  After laying off for seven weeks, they\\'re at it again.\\n\\nHis original troll was:\\nThese \"\"women\"\" were prostitutes; so much for the fwench \"\"pure wool\"\" garbage; nothing pure here. When your ancestors were prostitutes, as is the case of the fwench Canadians, it is curious how the decedants describe themselves as pure!\\n\\nJust to be sure there was no misunderstanding in the matter, I included a report from two of the three main reference works on les filles du roi that tells of one girl (one out of approximately 800) who was charged in Canada with prostitution (not a French prostitute, but a woman who was accused of falling into that life after arrival), and his comment is now:\\n\\nIf there is at least one prostitute among these fine young ladies then we are no longer speaking of \"\"Rumors and urban legends.\"\" The article claims only one was \"\"charged\"\"; that hardly aquits these pure-woolly \"\"women.\"\"\\n\\nHe also put in an NPOV marker and asked for a fact citation on the one story; it comes from two of the three books, and this is much too small an article for me to have to make individual citations when I list the three books as references.\\n\\nYou gave this person the trolling warning before, and he is violating it again.  I believe his IP range should be blocked.\\n\\n   \"',\n",
       " {'identity_hate': 0,\n",
       "  'insult': 0,\n",
       "  'obscene': 0,\n",
       "  'severe_toxic': 0,\n",
       "  'threat': 0,\n",
       "  'toxic': 0})"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "sample_row = val_df.iloc[20]\n",
    "sample_text = sample_row.comment_text\n",
    "sample_text_values = sample_row[LABEL_COLUMNS]\n",
    "sample_text, sample_text_values.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = TOKENIZER.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens = True,\n",
    "    max_length = 512,\n",
    "    padding=\"max_length\",\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask = True,\n",
    "    return_tensors = \"pt\",\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([  101,   107,  3605,  6919,  1863,  1113,  1624,   112,   188, 24645,\n",
       "          1118,   146,  1125,  1106,  2367,  1240,  1494,  1113,  1142,  1164]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "encoding.input_ids.squeeze()[:20], encoding.attention_mask.squeeze()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([  101,   107,  3605,  6919,  1863,  1113,  1624,   112,   188, 24645,\n",
       "         1118,   146,  1125,  1106,  2367,  1240,  1494,  1113,  1142,  1164])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "encoding.input_ids.flatten()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '\"',\n",
       " 'Van',\n",
       " '##dal',\n",
       " '##ism',\n",
       " 'on',\n",
       " 'King',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Daughters',\n",
       " 'by',\n",
       " 'I',\n",
       " 'had',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'your',\n",
       " 'help',\n",
       " 'on',\n",
       " 'this',\n",
       " 'about']"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "TOKENIZER.convert_ids_to_tokens(encoding.input_ids.squeeze()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index:int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        comment_text = data_row.comment_text\n",
    "        labels = data_row[LABEL_COLUMNS]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            max_length = 128,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            comment_text=comment_text,\n",
    "            input_ids = encoding.input_ids.flatten(),\n",
    "            attention_mask = encoding.attention_mask.flatten(),\n",
    "            labels = torch.FloatTensor(labels)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(train_df, TOKENIZER, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'comment_text': 'resuu u are all niggers \\n  lt',\n",
       " 'input_ids': tensor([  101,  1231,  6385,  1358,   190,  1132,  1155, 11437,  9146,  1116,\n",
       "           181,  1204,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'labels': tensor([1., 0., 0., 0., 0., 1.])}"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, val_Df, tokenizer, max_length=128, batch_size=8):\n",
    "        super(ToxicCommentsDataModule, self).__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = ToxicCommentsDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        self.val_dataset = ToxicCommentsDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "N_EPOCHS = 30\n",
    "\n",
    "datamodule=ToxicCommentsDataModule(train_df, val_df, TOKENIZER, max_length=128, batch_size=BATCH_SIZE)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes, steps_per_epoch, n_epochs):\n",
    "        super(ToxicCommentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
    "        self.classifier = (self.bert.config.hidden_size, n_classes)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "            return loss, output\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\" : output, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels= []\n",
    "        predictions = []\n",
    "\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "\n",
    "        for output in outputs:\n",
    "            for out_preds in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_preds)\n",
    "\n",
    "        labels = torch.stack(labels)\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        for i, name in enumerate(LABEL_COLUMNS):\n",
    "            roc_score = auroc(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", roc_score, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr = 2e-5)\n",
    "        warmup_steps = self.steps_per_epoch // 3\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            warmup_steps,\n",
    "            total_steps\n",
    "        )\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToxicCommentClassifier(\n",
    "    n_classes=6,\n",
    "    steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}